{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the input file\n",
    "\n",
    "- input file `predict_categories_dataset_ocrs.jsonl.gz` weights 335.8 MB  \n",
    "about the file : https://openfoodfacts.org/data/dataforgood2022/big/predict_categories_dataset_documentation.txt  download the file : https://openfoodfacts.org/data/dataforgood2022/big/predict_categories_dataset_ocrs.jsonl.gz\n",
    "\n",
    "- each line of the jsonl file contains OCRs associated with a barcode.\n",
    "- each OCR contains text, potentially in different languages\n",
    "\n",
    "## What is done in this notebook\n",
    "1. Make the notebook\n",
    "- A DataFrame is made from the jsonl.gz\n",
    "- All the OCRs of a product are concatenated\n",
    "- For each product the different languages are detected.  \n",
    "- When the text is not too long it remains as is, it is only cleaned with the `text_cleaner` there isnt' any text selection at all.\n",
    "\n",
    "Problem: in some case the text is too long. We need a way to extract only relevant words\n",
    "\n",
    "2. Selection strategies:\n",
    "- only the main language is kept (i.e the lang in which there are many words and a good confidence score)\n",
    "- TFIDF Selection strategy : The approach here is to filter by language and to create a TFIDF matrix. For each product of a given language we keep only the N-top words according to their TFIDF score.\n",
    "- language intersection strategy:\n",
    "We add words that are found in all the different languages detected.\n",
    "- OCR intersection strategy:\n",
    "We add words that are found in all the different OCRs (i.e all the different images).\n",
    "- Big words on the images:\n",
    "Finally we add the biggest words in the images. They are found by choosing words that have the biggest bounding polygon (anchor detection box) for the least text. In other words we choose the words that have the biggest detection-box-size / character-length ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#- As there are many languages to work with, It is suggested to work with a Multilanguage sentence transformer.\n",
    "#- As the sentences can't properly be identified on a product we behave like there is only one big sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ocrs = os.path.abspath(\"../datasets/predict_categories_dataset_ocrs.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_from_json_for_df(json_line: dict) -> list:\n",
    "    \"\"\" extract items from json and returns a row to put in a dataframe \n",
    "    Parameters\n",
    "    ----------\n",
    "    json_line: dict \n",
    "        dictionnary loaded from a line in the jsonl file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    row: list\n",
    "        list with items [code, ocr_texts, keys]\n",
    "            code: barcode of the product\n",
    "            ocr_texts: concatenated texts coming from all the images\n",
    "            keys: keys of all the images; for example 004/150/000/7229/2.json  <-- \"2\" is the key.\n",
    "    \"\"\"\n",
    "    code = json_line['code']\n",
    "    if \"ocrs\" in json_line:\n",
    "        texts = []\n",
    "        keys =  list(json_line['ocrs'].keys())\n",
    "        for key in keys:\n",
    "            ocr_text = json_line['ocrs'][key]['text']\n",
    "            texts.append(ocr_text)\n",
    "    row = [code, \" \".join(texts), keys]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_from_json_for_dict(json_line: dict, ocr_text_dict: dict) -> dict:\n",
    "    \"\"\" extracts ocr text from all images, for a given barcode.\n",
    "    returns a dict with new elements coming from json_line.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    json_line: dict \n",
    "        dictionnary loaded from a line in the jsonl file.\n",
    "    ocr_text_dict: dict\n",
    "        dictionnary in the following format {barcode: {key: ocr_text}}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ocr_text_dict: dict\n",
    "        dictionnary ocr_text_dict with new elements coming from json_line.\n",
    "    \"\"\"\n",
    "    \n",
    "    code = json_line['code']\n",
    "    if \"ocrs\" in json_line:\n",
    "        keys =  list(json_line['ocrs'].keys())\n",
    "        if len(keys) >0:\n",
    "            ocr_text_dict[str(code)] = {}\n",
    "            for key in keys:\n",
    "                ocr_text_dict[str(code)][key] = json_line['ocrs'][key]['text']\n",
    "    return ocr_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artificial_sentence_split(text:str, n_words_per_sentence:int = 8)-> list['str']: \n",
    "    \"\"\"splits artificially a text, based on a pre-defined number of words. \n",
    "    On average there are 15 words per sentence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str \n",
    "        input text.\n",
    "    n_words_per_sentence: int\n",
    "        number of words in each artificial sentence.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence_split: list\n",
    "        a list of sentences containing 8 words each.\n",
    "    \"\"\"\n",
    "    txt_split = text.split()\n",
    "    total_words = len(txt_split)\n",
    "    if total_words >= n_words_per_sentence:\n",
    "        n_sentences = total_words // n_words_per_sentence\n",
    "        rest = total_words % n_words_per_sentence\n",
    "        chunks = [[i*n_words_per_sentence, (i+1)*n_words_per_sentence ] for i in range(n_sentences)]\n",
    "        chunks[-1][1]+=rest\n",
    "    else:\n",
    "        chunks = [[0, total_words]]\n",
    "    sentence_split = [\" \".join(txt_split[slice(*chunk)]) for chunk in chunks]\n",
    "    return sentence_split\n",
    "\n",
    "\n",
    "def get_clean_lists_from_fasttext(lang_labels: list['list'], probs_list: list['list']):\n",
    "    \"\"\" fasttexts returns a list of lists for language labels and probs_list.\n",
    "    for each item in the list the argmax is taken\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lang_labels: list of lists .\n",
    "        list of langages found, for each OCR text.\n",
    "    probs_list: list of lists.\n",
    "        list of all the probabilities of the languages, for each OCR text.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lang_labels_output: list\n",
    "        list containing the main language for each OCR.\n",
    "    prob_list_output: list\n",
    "        The associated probabilities of the languages in lang_labels_output.\n",
    "    \"\"\"\n",
    "    main_lang_idx = [np.argmax(items) for items in probs_list]\n",
    "    lang_labels_output = [lang_label[i].split('__label__')[1] for lang_label, i in zip(lang_labels, main_lang_idx)]\n",
    "    prob_list_output = [prob_list[i] for prob_list,i in zip(probs_list, main_lang_idx)]\n",
    "    return lang_labels_output, prob_list_output\n",
    "\n",
    "\n",
    "def text_lang_split(text:str)-> dict:\n",
    "    \"\"\"\n",
    "    takes text as input and splits it in a dictionnary with languages found as keys.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    main_lang: str\n",
    "        language that contains the most words in the dictionnary\n",
    "    sorted_dict: dict:\n",
    "        dictionnary sorted in the following format : {\"lang\": {\"prob\": prob, \"len_text\":len_text, \"text\": text}}\n",
    "        text: text found with in a given language.\n",
    "        len_text: the length of the text.\n",
    "        prob: a list of probabilities, each probability corresponds to a sentence.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(\"\\.+\", \".\", text)\n",
    "    lang_dict = {}\n",
    "    sentences = artificial_sentence_split(text)\n",
    "    langs, probs = get_langs(sentences)\n",
    "    for sentence, lang, prob in zip(sentences, langs, probs):\n",
    "       \n",
    "        if lang in lang_dict:\n",
    "            lang_dict[lang][\"prob\"].append(prob)\n",
    "            lang_dict[lang][\"len_text\"] += len(sentence)\n",
    "            lang_dict[lang][\"text\"]+= \" \" + sentence\n",
    "            \n",
    "        else:\n",
    "            lang_dict[lang] = {\"prob\": prob}\n",
    "            lang_dict[lang][\"len_text\"] = len(sentence)\n",
    "            lang_dict[lang][\"text\"] = sentence\n",
    "            \n",
    "    sorted_dict = {k: v for k, v in sorted(lang_dict.items(), key=lambda item: item[1][\"len_text\"], reverse = True)}\n",
    "    main_lang = next(iter(sorted_dict))\n",
    "    return main_lang, sorted_dict\n",
    "\n",
    "def get_langs(sentences:list):\n",
    "    \"\"\" detects languages and returns for each sentence the most probable one with its associated probability.\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: list\n",
    "        list of string sentences.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    langs: list \n",
    "        list of languages.\n",
    "    probs: list\n",
    "        list of proababilities.\n",
    "    \"\"\"\n",
    "    lang_labels, probs_list = model.predict(sentences)\n",
    "    langs, probs = get_clean_lists_from_fasttext(lang_labels, probs_list)\n",
    "    return langs, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make DataFrame and ocr_text_dict from jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(790383, 3)\n",
      "597242\n"
     ]
    }
   ],
   "source": [
    "\"\"\"approx time 15sec\n",
    "Iterating through the jsonl to extract elements for the dataframe nad the ocr dictionnary.\n",
    "\"\"\"\n",
    "from IPython.display import clear_output\n",
    "import gzip\n",
    "import json\n",
    "# make df from json\n",
    "rows = []\n",
    "ocr_text_dict = {}\n",
    "with gzip.open(path_ocrs) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 10000:\n",
    "            json_line = json.loads(line)\n",
    "            if len(str(json_line['code'])) == 13:\n",
    "                #for df\n",
    "                row = get_row_from_json_for_df(json_line)\n",
    "                rows.append(row)\n",
    "                #for dict\n",
    "                ocr_text_dict = get_item_from_json_for_dict(json_line, ocr_text_dict)\n",
    "\n",
    "df = pd.DataFrame(rows, columns = [\"code\", \"texts\", \"keys\"]).drop_duplicates(subset = \"code\")\n",
    "df['code'] = df['code'].astype(str)\n",
    "df['texts'] = df['texts'].astype(str)\n",
    "print(df.shape)\n",
    "print(len(ocr_text_dict))\n",
    "\n",
    "del rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing non normalized codes (790383, 3)\n",
      "after removing short texts (596550, 3)\n"
     ]
    }
   ],
   "source": [
    "is_real_ean = df['code'].str.len() == 13\n",
    "df = df[is_real_ean]\n",
    "print(\"after removing non normalized codes\", df.shape)\n",
    "### Keep only texts > 10 char\n",
    "len_sup_10 = (df[\"texts\"].str.len()> 10)\n",
    "df = df[len_sup_10]\n",
    "print(\"after removing short texts\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract main language text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each text associated with a barcode, there are potentially many languages used to describe the product.\n",
    "The aim of this section is to find the main language and to extract its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "PRETRAINED_MODEL_PATH = '../datasets/lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "clear_output()\n",
    "\n",
    "def get_lang_items_from_pd_textlist(pdSeries) -> list:\n",
    "    \"\"\" takes a text as input, creates sentences, detects the most proable languages\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdSeries: pd.Series\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text_list: list\n",
    "        each item is the text associated with the most probable language. \n",
    "    lang_dict_list: list\n",
    "        each item is a dictionnary that splits the text according to the languages found.\n",
    "    main_lang_list: list\n",
    "        each item is the most probable language for a given text.\n",
    "\n",
    "    \"\"\"\n",
    "    text_list= []\n",
    "    lang_dict_list = []\n",
    "    main_lang_list = []\n",
    "    for t in tqdm(pdSeries):\n",
    "        sentences = artificial_sentence_split(t)\n",
    "        #detect and split text in a dict according to the languages found.\n",
    "        main_lang, lang_dict = text_lang_split(t)\n",
    "        text_to_keep = lang_dict[main_lang][\"text\"]\n",
    "        text_list.append(text_to_keep)\n",
    "        main_lang_list.append(main_lang)\n",
    "        lang_dict_list.append(lang_dict)\n",
    "    return text_list, lang_dict_list, main_lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 596550/596550 [02:06<00:00, 4721.20it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>texts</th>\n",
       "      <th>keys</th>\n",
       "      <th>text_main_lang</th>\n",
       "      <th>main_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0021065000071</td>\n",
       "      <td>Nutrition Facts\\n24 servings per container\\nSe...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nutrition Facts 24 servings per container Serv...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0021065000101</td>\n",
       "      <td>Includes 0g Added Sugars 0%\\nNutrition Facts\\n...</td>\n",
       "      <td>[2, 1]</td>\n",
       "      <td>Includes 0g Added Sugars 0% Nutrition Facts 71...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                              texts    keys  \\\n",
       "0  0021065000071  Nutrition Facts\\n24 servings per container\\nSe...     [1]   \n",
       "1  0021065000101  Includes 0g Added Sugars 0%\\nNutrition Facts\\n...  [2, 1]   \n",
       "\n",
       "                                      text_main_lang main_lang  \n",
       "0  Nutrition Facts 24 servings per container Serv...        en  \n",
       "1  Includes 0g Added Sugars 0% Nutrition Facts 71...        en  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"approx time 2min\"\"\"\n",
    "text_list, lang_dict_list, main_lang_list = get_lang_items_from_pd_textlist(df['texts'])\n",
    "main_lang_dict = {code: dic for (code,dic) in zip(df[\"code\"], lang_dict_list)}\n",
    "#assign new items to df\n",
    "df[\"text_main_lang\"] = text_list\n",
    "df[\"main_lang\"] = main_lang_list\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text:str) -> str:\n",
    "  \"\"\" takes a text as input and cleans it\"\"\"\n",
    "\n",
    "  dont_take = [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "  text_cleaned = text.replace(\"\\n\", \" \") #remove line breaks\n",
    "  text_cleaned = re.sub(\"\\S*(www\\.|\\.com|\\.net|\\.fr|\\.co\\.uk|\\.org)\\S*\", \"\", text_cleaned) #remove websites\n",
    "  text_cleaned = re.sub(\"\\w*([0-9]{0,}[,|\\.]{0,}[0-9])\\w*\", \" \", text_cleaned) #remove measurements \n",
    "  text_cleaned = re.sub(r\"\\b([a-zA-Z]{1})\\b\", \" \", text_cleaned) # remove isolated letters ex --> g g g g g\n",
    "  text_cleaned = re.sub(\"( +- +)\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\"[\\·|/|\\-|\\\\|(|)|\\+|\\*|\\[|\\]|™|ᴿˣ|\\*|\\—|\\^|\\\"|®|>|<|″|\\||\\&|\\#|\\,|\\;|⭐|\\xa0|\\?|\\%|\\'|©|\\@|\\$|\\€|\\:|\\}|\\{|\\°]\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\" +\", \" \", text_cleaned) # remove multiple spaces\n",
    "\n",
    "  text_cleaned = \" \".join([w for w in text_cleaned.split() if (w.isalpha() and w.lower() not in dont_take)])\n",
    "  return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def parallel_calc(iterable, func, n_core = mp.cpu_count()):\n",
    "    \"\"\" simple wrapper code around func to parallelize the work.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable: iterable\n",
    "        items to feed the parralelized workers\n",
    "    func: callable\n",
    "        function to use for parallelization \n",
    "    n_core: int\n",
    "        total CPUs on your computer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results: list\n",
    "        iterable processed by the function. \n",
    "    \"\"\"\n",
    "    pool = mp.Pool(n_core-1)\n",
    "    results = pool.map(func, np.array(iterable))\n",
    "    pool.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"approx time 2m\"\"\"\n",
    "#df[\"text_cleaned\"] = df[\"text_main_lang\"].apply(lambda x: text_cleaner(x))\n",
    "#df = df.set_index(\"code\", drop = True)\n",
    "\n",
    "\"\"\"approx time 30sec\"\"\"\n",
    "# simple wrapper code around text_cleaner to parallelize the work\n",
    "df[\"text_cleaned\"] = parallel_calc(df[\"text_main_lang\"], text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"approx time 1min\"\"\"\n",
    "df[\"original_text_cleaned\"]  = parallel_calc(df[\"texts\"], text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>texts</th>\n",
       "      <th>keys</th>\n",
       "      <th>text_main_lang</th>\n",
       "      <th>main_lang</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0021065000071</td>\n",
       "      <td>Nutrition Facts\\n24 servings per container\\nSe...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Nutrition Facts 24 servings per container Serv...</td>\n",
       "      <td>en</td>\n",
       "      <td>Nutrition Facts servings container Serving Siz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0021065000101</td>\n",
       "      <td>Includes 0g Added Sugars 0%\\nNutrition Facts\\n...</td>\n",
       "      <td>[2, 1]</td>\n",
       "      <td>Includes 0g Added Sugars 0% Nutrition Facts 71...</td>\n",
       "      <td>en</td>\n",
       "      <td>Includes Added Sugars Nutrition Facts servings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0021078019091</td>\n",
       "      <td>SOURCE NATURALS\\nDIETARY SUPPLEMENT\\nTRUE WHEY...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>SOURCE NATURALS DIETARY SUPPLEMENT TRUE WHEY T...</td>\n",
       "      <td>en</td>\n",
       "      <td>SOURCE NATURALS DIETARY SUPPLEMENT TRUE WHEY T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                              texts    keys  \\\n",
       "0  0021065000071  Nutrition Facts\\n24 servings per container\\nSe...     [1]   \n",
       "1  0021065000101  Includes 0g Added Sugars 0%\\nNutrition Facts\\n...  [2, 1]   \n",
       "9  0021078019091  SOURCE NATURALS\\nDIETARY SUPPLEMENT\\nTRUE WHEY...     [1]   \n",
       "\n",
       "                                      text_main_lang main_lang  \\\n",
       "0  Nutrition Facts 24 servings per container Serv...        en   \n",
       "1  Includes 0g Added Sugars 0% Nutrition Facts 71...        en   \n",
       "9  SOURCE NATURALS DIETARY SUPPLEMENT TRUE WHEY T...        en   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  Nutrition Facts servings container Serving Siz...  \n",
       "1  Includes Added Sugars Nutrition Facts servings...  \n",
       "9  SOURCE NATURALS DIETARY SUPPLEMENT TRUE WHEY T...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "word_count = df['text_cleaned'].str.split().str.len()\n",
    "sns.histplot(word_count)\n",
    "plt.xlim([0,1000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_to_word_dict(vectorizer):\n",
    "    \"\"\" make the dictionnary index_to_word\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorizer: sklearn.feature_extraction.text.TfidfVectorizer\n",
    "        tfidf_vectorizer instance fit on a sub_dataframe. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index_to_word: dict\n",
    "        maps a word index (vectorizer.vocabulary_) to its associated word.\n",
    "    \"\"\"\n",
    "    index_to_word = {index: word for index, word in zip(vectorizer.vocabulary_.values(), vectorizer.vocabulary_.keys())}\n",
    "    return index_to_word\n",
    "\n",
    "def get_words_with_highest_scores(scores, words, n_words_to_take):\n",
    "    \"\"\"\n",
    "    takes scores and words of an OCR as inputs and returns \n",
    "    words with highest tfidf scores\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores: list\n",
    "        list of tfidf scores.\n",
    "    words: list\n",
    "        list of words\n",
    "    n_words_to_take: int\n",
    "        number of words to select.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_items: list\n",
    "        list of words with highest scores.\n",
    "    items: list:\n",
    "        all items\n",
    "    \"\"\"\n",
    "    items = [item for item in sorted(zip(scores, words), reverse = True)]\n",
    "    best_items = items[:n_words_to_take]\n",
    "    return best_items, items\n",
    "\n",
    "\n",
    "def get_words_and_scores_from_tfidf_matrix(doc, cols, tfidf_matrix, index_to_word):\n",
    "    \"\"\"takes a document (line of tfidf matrix) and its words (columns of tfidf matrix\n",
    "    and returns words of the documents with their ifidf scores\n",
    "    columns are the non zero values of the tfidf matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: int\n",
    "        document index.\n",
    "    cols: int\n",
    "        column indexes.\n",
    "    tfidf_matrix: scipy.sparse.csr.csr_matrix\n",
    "        tdidf sparse matrix.\n",
    "    index_to_word: dict\n",
    "        maps a word index (vectorizer.vocabulary_) to its associated word.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_items: list\n",
    "        list of words with highest scores.\n",
    "    items: list:\n",
    "        list of all words.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    words = []\n",
    "    for col in cols:\n",
    "        word = index_to_word[col] \n",
    "        score = tfidf_matrix[doc, col]\n",
    "        words.append(word)\n",
    "        scores.append(score)\n",
    "    return words, scores\n",
    "\n",
    "def text_selection(df_idx, sub_df_idx,  index_to_word, tfidf_matrix, n_words_to_take = 50):\n",
    "    \"\"\" makes the text selection on a document based on the tfidf score\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_idx: int\n",
    "        index of the document in the original dataframe\n",
    "    sub_df_idx: int\n",
    "        index of the document in the sub-dataframe (filtered dataframe on a given language)\n",
    "    index_to_word: dict\n",
    "        maps a word index (vectorizer.vocabulary_) to its associated word.\n",
    "    tdidf_matrix: scipy.sparse.csr.csr_matrix\n",
    "        tdidf sparse matrix for a given language.\n",
    "    n_words_to_take: int\n",
    "        total words to keep.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text_selection: str\n",
    "        text selected after the tfidf trick.\n",
    "    items: list\n",
    "        list where each element is a tuple in the following format (tdidf_score, word).\n",
    "        All the words in the original text are kept.\n",
    "    \"\"\"\n",
    "    rows, cols = tfidf_matrix[sub_df_idx].nonzero()\n",
    "    ## extract words and scores from tfidf matrix\n",
    "    words, scores = get_words_and_scores_from_tfidf_matrix(sub_df_idx, cols, tfidf_matrix, index_to_word)\n",
    "    #extract words with highest score from sentence\n",
    "    best_items, items = get_words_with_highest_scores(scores, words, n_words_to_take)\n",
    "    best_words = [item[1] for item in best_items]\n",
    "    \n",
    "    text_selection = \" \".join([word for word in df[\"text_cleaned\"].iloc[df_idx].split() if str(word).lower() in best_words])\n",
    "    text_selection = \" \".join(best_words)\n",
    "    #text_selection_unique = remove_duplicates(text_selection)\n",
    "    return text_selection, items\n",
    "\n",
    "def text_selection_from_Series(text_Series, tfidf_matrix, index_to_word):\n",
    "    \"\"\" takes a text pd.Series as input and makes the tfidf-selection for each document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_Series: pd.Series\n",
    "        pd.Series containing OCR text\n",
    "    tdidf_matrix: scipy.sparse.csr.csr_matrix\n",
    "        tdidf sparse matrix for a given language.\n",
    "    index_to_word: dict\n",
    "        maps a word index (vectorizer.vocabulary_) to its associated word.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text_selection_list: list\n",
    "        list containing all the texts after the tfidf-selection\n",
    "    items_list: list\n",
    "        each element of items_list is a list containing items.\n",
    "        An item is in the following format (tdidf_score, word)\n",
    "        All the words in the original text are kept.\n",
    "    \"\"\"\n",
    "    text_selection_list = []\n",
    "    items_list = []\n",
    "    for sub_df_idx, df_idx in enumerate(tqdm(text_Series.index)):\n",
    "        text_selection_unique, items = text_selection(df_idx, sub_df_idx, index_to_word, tfidf_matrix, n_words_to_take = 30)\n",
    "        text_selection_list.append(text_selection_unique)\n",
    "        items_list.append(items)\n",
    "    return text_selection_list, items_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text, get_keys = False):\n",
    "    \"\"\"takes a text and removes the duplicated words. It keeps the string's original casing\"\"\"\n",
    "    D = {word.lower(): word  for word in str(text).split()}\n",
    "    if get_keys:\n",
    "        return \" \".join(D.keys())\n",
    "    else:\n",
    "        return \" \".join(D.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing TFIDF Trick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tfidf_selection\"] = \"\"\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"takes approx 6min\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "problematic_langs = []\n",
    "df = df.reset_index(drop = True)\n",
    "for lang in df[\"main_lang\"].unique():\n",
    "    print(\"lang:\", lang)\n",
    "    lang_filter = df[\"main_lang\"]== lang \n",
    "    sub_df = df[lang_filter]\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(min_df = 1, max_df = 0.8)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sub_df[\"text_cleaned\"])\n",
    "        index_to_word = get_index_to_word_dict(vectorizer)\n",
    "        text_selection_list, items_list =  text_selection_from_Series(sub_df[\"text_cleaned\"], tfidf_matrix, index_to_word)\n",
    "        df.loc[lang_filter, \"tfidf_selection\"] = text_selection_list\n",
    "    except:\n",
    "        problematic_langs.append(lang)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problematic_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"tfidf_selection\"] == \"\",:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection words trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_words_lang(lang_dict):\n",
    "    \"\"\"get words that are used in all languages, for a given text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lang_dict: dict\n",
    "        dictionnary in the following format {\"lang\": {\"prob\": prob, \"len_text\":len_text, \"text\": text}}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    intersection: str\n",
    "        words used in all languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    dont_take= [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "    if len(lang_dict) > 1:\n",
    "        sets = [set(remove_duplicates(text_cleaner(lang_dict[lang][\"text\"]), get_keys = True).split()) for lang in lang_dict.keys()]\n",
    "        intersection =  \" \".join([word for word in set.intersection(*sets) if word not in dont_take])\n",
    "        return intersection\n",
    "    else: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_words_ocr(barcode):\n",
    "    \"\"\"get words that are found in all images, for a given barcode.\n",
    "    Parameters\n",
    "    ----------\n",
    "    barcode: str\n",
    "        EAN - 13 digits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    intersection: str\n",
    "        words found in all images, for a given barcode.\n",
    "    \"\"\"\n",
    "    ocr_dict = ocr_text_dict[barcode]\n",
    "    dont_take= [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "    if len(ocr_dict) > 1:\n",
    "        sets = [set(remove_duplicates(text_cleaner(ocr_dict[key]), get_keys = True).split()) for key in ocr_dict.keys()]\n",
    "        intersection = \" \".join([word for word in set.intersection(*sets) if word not in dont_take])\n",
    "        return intersection\n",
    "    else: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14662308272567262\n"
     ]
    }
   ],
   "source": [
    "\"\"\"takes approx 1min30\"\"\"\n",
    "intersection_list_lang = parallel_calc(lang_dict_list, get_intersect_words_lang)\n",
    "print(len([item for item in intersection_list_lang if item != \"\"]) / len(intersection_list_lang))\n",
    "#pd.DataFrame(zip(df[\"code\"], intersection_list), columns = [\"barcode\", \"intersection\"]).to_csv('results_intersections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4644321515380102\n"
     ]
    }
   ],
   "source": [
    "\"\"\"takes approx 56sec\"\"\"\n",
    "intersection_list_ocr = parallel_calc(df[\"code\"], get_intersect_words_ocr)\n",
    "print(len([item for item in intersection_list_ocr if item != \"\"]) / len(intersection_list_ocr))\n",
    "#pd.DataFrame(zip(df[\"code\"], intersection_list), columns = [\"barcode\", \"intersection\"]).to_csv('results_intersections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"intersection_words_lang\"] = intersection_list_lang\n",
    "df[\"intersection_words_ocr\"] = intersection_list_ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch big words from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import urllib\n",
    "import io\n",
    "\n",
    "def make_barcode(x):\n",
    "    \"\"\" takes an EAN of 13 digits and returns an EAN in the format \"XXX/XXX/XXX/XXXX \"\"\"\n",
    "    x = str(x)\n",
    "    return \"{}/{}/{}/{}\".format(x[:3], x[3:6], x[6:9], x[9:])\n",
    "\n",
    "def make_link_from_barcode(barcode, df, file = \"image\", keys = None):\n",
    "    \"\"\"creates url for json or jpg, from the barcode \"\"\"\n",
    "    if keys is None:\n",
    "        keys = df.loc[df[\"code\"]==barcode, \"keys\"].values[0]\n",
    "    if isinstance(keys, str):\n",
    "        keys = eval(keys)\n",
    "    elif isinstance(keys, list):\n",
    "        pass\n",
    "\n",
    "    links = []\n",
    "    if file == \"image\": file = \"jpg\"\n",
    "    if file == \"json\": file = \"json\"\n",
    "    barcode_with_slash = make_barcode(barcode)\n",
    "    for key in keys:\n",
    "        link = \"https://world.openfoodfacts.org/images/products/{}/{}.{}\".format(barcode_with_slash, key,file)\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def show_images(links):\n",
    "    \"\"\"display images from links\"\"\"\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        image_bytes = io.BytesIO(response.content)\n",
    "        img = Image.open(image_bytes)\n",
    "        img.show()\n",
    "\n",
    "\n",
    "\n",
    "def show_images_from_barcode(barcode, df, keys = None):\n",
    "    \"\"\"display all the images belonging to a barcode\"\"\"\n",
    "    links = make_link_from_barcode(barcode, df=df, keys = keys)\n",
    "    show_images(links)\n",
    "\n",
    "import math\n",
    "import requests\n",
    "def get_score_from_verticles(txt_annotations:dict):\n",
    "    \"\"\"the code iterates through all the anchor boxes used to detect the text.\n",
    "    It finds the bigest Anchor box with the least words. The idea here is to find important words\n",
    "    and usually important words are written in big print and are titles or captions so it doesn't have \n",
    "    much characters. The score computed here is detection_box_area / len_text \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt_annotations: dict\n",
    "        dictionnary containing detection boxes coordinates for a given image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "        highest score through iteration.\n",
    "    text: str\n",
    "        text found in the anchor associated with the highest score. \n",
    "    \"\"\"\n",
    "    txt = txt_annotations[\"description\"]\n",
    "    len_text = len(txt)\n",
    "    y_min = math.inf\n",
    "    y_max = -math.inf\n",
    "    x_min = math.inf\n",
    "    x_max = -math.inf\n",
    "\n",
    "    verticles = txt_annotations['boundingPoly']['vertices']\n",
    "    for coords in verticles:\n",
    "        if 'y' in coords:\n",
    "            y_min = min(coords['y'], y_min)\n",
    "            y_max = max(coords['y'], y_max)\n",
    "        if 'x' in coords:\n",
    "            x_min = min(coords['x'], x_min)\n",
    "            x_max = max(coords['x'], x_max)\n",
    "    area = abs(x_max-x_min) * abs(y_max-y_min)\n",
    "    score = area/len_text\n",
    "    return score,txt\n",
    "\n",
    "def get_n_most_important_words(results, word_count_limit = 10):\n",
    "    \"\"\"selects N unique words from an image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results: list:\n",
    "        list of tuples, the tuples are in the following format: (score, text)\n",
    "        \n",
    "    word_count_limit: int\n",
    "        number of words to select.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    words_to_keep: str\n",
    "        N words to keep from an image\n",
    "    \"\"\"\n",
    "    to_keep = {}\n",
    "    for items in results:\n",
    "        words = items[1]\n",
    "        for word in text_cleaner(words).split():\n",
    "            if word not in to_keep:\n",
    "                to_keep[word.lower()] = word\n",
    "                if len(to_keep) == word_count_limit:\n",
    "                    return \" \".join(to_keep.values())\n",
    "    words_to_keep =  \" \".join(to_keep.values())\n",
    "    return words_to_keep      \n",
    "\n",
    "\n",
    "\n",
    "def get_big_words_from_txt_annotations(txt_annotations):\n",
    "    \"\"\" selects N unique words from an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt_annotations: dict\n",
    "        dictionnary containing detection boxes coordinates for a given image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    words_to_keep: str\n",
    "        N words to keep\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = sorted([get_score_from_verticles(txt_a) for txt_a in txt_annotations], reverse = True)\n",
    "        text = get_n_most_important_words(results, word_count_limit = 10)\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_big_words_from_image(barcode, df, keys = None):\n",
    "    \"\"\" selects N unique words from an image.Here, we use the url of the json, created with the barcode\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    barcode: str\n",
    "        EAN - 13 digits\n",
    "    df: pd.DataFrame\n",
    "    keys: list, default = None\n",
    "        list of keys used to create the urls. if the value is None,\n",
    "        the information is fetched from the dataframe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words_to_keep: str\n",
    "        N words to keep    \n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    links = make_link_from_barcode(barcode, df, file = \"json\", keys = keys)\n",
    "    for link in links:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(link)\n",
    "            js = json.loads(response.read())\n",
    "            txt_annotations = js['responses'][0]['textAnnotations'][0]\n",
    "            results = sorted([get_score_from_verticles(txt_annotations)  for txt_annotations in js['responses'][0]['textAnnotations']], reverse = True)\n",
    "            text = get_n_most_important_words(results, word_count_limit = 15)\n",
    "\n",
    "        except:\n",
    "            text = \"\"\n",
    "        texts.append(text)\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def get_big_words_from_image_clean(barcode, df, keys = None):\n",
    "    return text_cleaner(get_big_words_from_image(barcode, df, keys = keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_words_dict = {}\n",
    "i_last = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "250000\n",
      "500000\n",
      "750000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"takes approx 7m30\"\"\"\n",
    "txt_annotations_path = \"../datasets/txtannotations.jsonl.gzip\"\n",
    "import gzip\n",
    "with gzip.open(txt_annotations_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > i_last:\n",
    "            i_last +=1\n",
    "            json_str = line.decode('utf-8')\n",
    "            txt_annotations = json.loads(json_str)\n",
    "            barcode = list(txt_annotations.keys())[0]\n",
    "            if str(barcode) in big_words_dict:\n",
    "                sentences_list = [get_big_words_from_txt_annotations(txt_annotations[barcode][key]) for key in sorted(txt_annotations[barcode].keys())]\n",
    "                if str(barcode) == '8480000141927':\n",
    "                    problem = sentences_list\n",
    "                    problem_keys = txt_annotations[barcode].keys()\n",
    "                if len(big_words_dict[str(barcode)]) <= 500:\n",
    "                    big_words_dict[str(barcode)] +=  \" \" + \" \".join(sentences_list)\n",
    "            else:\n",
    "                sentences_list = [get_big_words_from_txt_annotations(txt_annotations[barcode][key]) for key in sorted(txt_annotations[barcode].keys())]\n",
    "                big_words_dict[str(barcode)] = \" \".join(sentences_list)\n",
    "\n",
    "        if i %250000 == 0:\n",
    "            print(i)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363547"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375415"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"barcodes_dict_with_new_ones.pkl\"\n",
    "with open(path, 'rb') as file:\n",
    "    big_words_dict = pickle.load(file)\n",
    "len(big_words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"big_words\"] = df[\"code\"].apply(lambda x: big_words_dict[str(x)] if str(x) in big_words_dict else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142417    140\n",
       "141865    112\n",
       "85698     110\n",
       "157713    110\n",
       "190957    110\n",
       "         ... \n",
       "448646      0\n",
       "448647      0\n",
       "448648      0\n",
       "448650      0\n",
       "596549      0\n",
       "Name: big_words, Length: 596550, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"big_words\"].str.split().str.len().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make output df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_selection\"] = (\n",
    "    df[\"big_words\"].astype(str) + \". \" +\n",
    "    df[\"intersection_words_ocr\"].fillna(\"\").astype(str) + \". \" +\n",
    "    df[\"intersection_words_lang\"].fillna(\"\").astype(str) + \". \" +\n",
    "    df.apply(\n",
    "        lambda x: x[\"original_text_cleaned\"] if len(x[\"original_text_cleaned\"]) < 500 \n",
    "        else x[\"text_cleaned\"]if len(x[\"text_cleaned\"]) < 500\n",
    "        else (remove_duplicates(x[\"text_cleaned\"]) if len(remove_duplicates(x[\"text_cleaned\"]).split()) < 80\n",
    "        else x[\"tfidf_selection\"]) ,axis = 1).astype(str)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise or empty lines: 194\n",
      "(596356, 12)\n"
     ]
    }
   ],
   "source": [
    "empty_or_noise = df[\"word_selection\"].str.len() < 10\n",
    "print(\"noise or empty lines:\", empty_or_noise.sum())\n",
    "df_final = df[~empty_or_noise]\n",
    "print(df_final.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check random lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise or empty lines: 194\n",
      "(596356, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"dataset.pkl\")\n",
    "df[\"code\"] = df[\"code\"].astype(str)\n",
    "\n",
    "empty_or_noise = df[\"word_selection\"].str.len() < 10\n",
    "print(\"noise or empty lines:\", empty_or_noise.sum())\n",
    "df_final = df[~empty_or_noise]\n",
    "print(df_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22627\n",
      "ancel FABRIQUE EN Alsace Savourez Véritables Grands Bretgels SANS COLORANT BLÉ ORGE malt Salés ingrédients Liste des Biscuits farine huile tournesol Huile Biscuits salés Liste des ingrédients farine BLÉ sel. huile. . Biscuits salés Liste des ingrédients farine BLÉ huile tournesol Sel extrait malt ORGE FADRIUE EN Alsace ancel Véritables Grands Bretzels SANS COLORANT CONSERVATEUR PALME Ssachets fraicheur so produit contient portions Par Portion valeurs nutritionnelles moyennes énergétique Matières Grasses acides gras saturés Glucides sucres Protéines Apport référence un adulte type Ces les peuvent varier selon age sexe activité\n",
      "89\n",
      "3027030010660\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(range(df_final.shape[0]))\n",
    "#i = 10157 # maitre coq volaille\n",
    "barcode = df_final[\"code\"].iloc[i]\n",
    "print(i)\n",
    "print(df_final[\"word_selection\"].iloc[i])\n",
    "print(len(df_final[\"word_selection\"].iloc[i].split()))\n",
    "print(barcode)\n",
    "show_images_from_barcode(barcode, df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valeurs nutritionnelles moyennes gr acides gras acides gras monoinsaturés acides greenwooz HUILE'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[\"text_cleaned\"].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nestle Cerealo Ptite PÉPITES FONDANTES Chocplat Biscuite au lait sans CE ot avae Bcide ENFANTS blé en longtemps Nestlé variée'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_words_dict[barcode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3325815010738\n",
      "Rillettes de Thon Ingredients Thon 52%, huile de colza, eau, poudre de blanc d'oeuf (sol), protéines de lait, fibres de pommes de terre, sel, épaississants farne de graines de caroube, carraghénanes Valeurs nutritionnelles moyennes pour 100g Energie 1163K 281Kcal, Matiéres Grasses 24g dont acides gras saturés 1,8g, Glucides 0,6g dont sucres <0,5g; Protéines 15g; Sel 1,0g A consommer de préférences avant le: voir sur le coté de la boîte Apres ouverture, conserver au réfrigérateur et à consommer dans les deux jours. SAS les Délices de la mer ZA des 4 voies 22170 Plélo Poids Net: 80g FR 22.182.005 CE 3325815 010738  Rillettes de Thon Ingredients Thon 52 % , huile de colza, eau, poudre de blanc d'oeuf (sol), protéines de lait, ibres de pommes de terre, sel, épaississants farine de graines de caroube, camraghénanes. Valeurs nutritionnelles moyennes pour 100g Energie 1163K 281Kcal; Matiéres Grasses : 24g dont acides gras saturés 1,8g, Glucides 0,6g dont sucres <0,5g, Protéines 15g, Sel 1,0g A consommer de préférences avant le: voir sur le coté de la boîte Après ouverture, conserver au réfrigérateur et à consommer dans les deux jours. SAS les Délices de la mer ZA des 4 voies 22170 Plélo Poids Net: 80g FR 22.182.005 CE 3 325815 010738 \n",
      "____ final selection ____\n",
      "Rillettes Thon huile colza eau poudre blanc oeuf sol protéines Rillettes Thon huile colza eau poudre blanc oeuf sol protéines. lait boîte glucides huile pommes saturés thon blanc eau réfrigérateur des fr mer nutritionnelles voir poudre épaississants sur préférences grasses coté za plélo conserver voies sucres moyennes energie à acides gras rillettes ce valeurs sel ouverture caroube délices sas au terre sol matiéres les protéines graines deux oeuf dans colza avant. fr ce thon rillettes. Rillettes Thon huile colza eau poudre blanc oeuf sol Protéines lait fibres pommes terre Sel épaississants farne graines caroube carraghénanes Valeurs nutritionnelles moyennes Energie Matiéres Grasses acides gras saturés Glucides sucres préférences avant voir sur coté boîte Apres ouverture conserver au réfrigérateur à dans les deux SAS Délices mer ZA des voies Plélo ibres farine Après FR CE\n",
      "____ text after cleaning ____\n",
      "Rillettes Thon Thon huile colza eau poudre blanc oeuf sol protéines lait fibres pommes terre sel épaississants farne graines caroube carraghénanes Valeurs nutritionnelles moyennes Energie Matiéres Grasses acides gras saturés Glucides sucres Protéines Sel préférences avant voir sur coté boîte Apres ouverture conserver au réfrigérateur à dans les deux SAS les Délices mer ZA des voies Plélo Thon huile colza eau poudre blanc oeuf sol protéines lait ibres pommes terre sel épaississants farine graines caroube Valeurs nutritionnelles moyennes Energie Matiéres Grasses acides gras saturés Glucides sucres Protéines Sel préférences avant voir sur coté boîte Après ouverture conserver au réfrigérateur à dans les deux SAS les Délices mer ZA des voies Plélo FR CE\n",
      "____ text selection with tdidf score\n",
      "plélo préférences voies thon matiéres délices sol coté boîte deux za caroube épaississants sas mer les pommes oeuf farne réfrigérateur ibres terre blanc graines colza protéines rillettes ouverture sel poudre\n",
      "___ text selection with intersection between languages words lang\n",
      "fr ce thon rillettes\n",
      "___ text selection with intersection between languages words ocr\n",
      "lait boîte glucides huile pommes saturés thon blanc eau réfrigérateur des fr mer nutritionnelles voir poudre épaississants sur préférences grasses coté za plélo conserver voies sucres moyennes energie à acides gras rillettes ce valeurs sel ouverture caroube délices sas au terre sol matiéres les protéines graines deux oeuf dans colza avant\n",
      "___ text selection - big words from image\n",
      "Rillettes Thon huile colza eau poudre blanc oeuf sol protéines Rillettes Thon huile colza eau poudre blanc oeuf sol protéines\n"
     ]
    }
   ],
   "source": [
    "#5000354914829\n",
    "#3011360020178\n",
    "#3278910707327\n",
    "#5400141165043 151247\n",
    "#3083681008616\n",
    "import numpy as np\n",
    "#18546\n",
    "#i = 63663\n",
    "#i = np.random.choice(range(df_final.shape[0]))\n",
    "#i = 18546\n",
    "input_text = df_final[\"texts\"].iloc[i]\n",
    "final_selection = df_final[\"word_selection\"].iloc[i]\n",
    "text_clean = df_final[\"text_cleaned\"].iloc[i]\n",
    "text_tf = df_final[\"tfidf_selection\"].iloc[i]\n",
    "text_insct_lang = df_final[\"intersection_words_lang\"].iloc[i]\n",
    "text_insct_ocr = df_final[\"intersection_words_ocr\"].iloc[i]\n",
    "barcode = df_final[\"code\"].iloc[i]\n",
    "\n",
    "\n",
    "print(barcode)\n",
    "print(re.sub(r\"\\n\", \" \", input_text))\n",
    "print(\"____ final selection ____\")\n",
    "print(final_selection)\n",
    "print(\"____ text after cleaning ____\")\n",
    "print(text_clean)\n",
    "print(\"____ text selection with tdidf score\")\n",
    "print(text_tf)\n",
    "print(\"___ text selection with intersection between languages words lang\")\n",
    "print(text_insct_lang)\n",
    "print(\"___ text selection with intersection between languages words ocr\")\n",
    "print(text_insct_ocr)\n",
    "print(\"___ text selection - big words from image\")\n",
    "if barcode in big_words_dict:\n",
    "    print(big_words_dict[barcode])\n",
    "#show_images_from_barcode(barcode, df = df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valeurs nutritionnelles moyennes gr Matières acides gras acides gras monoinsaturés acides gras greenwooz HUILE TOURNESOL VIERGE BI PRESSION À FROID MADE IN FRANCE'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information nutritionnelle Energie Matières grasses acides gras saturés Glucides sucres PRINCE CHOCOLAT NOUV ELLI RECETE Ence re illeure AU BLE PRINCE CHAR CHOCOLAT au blé complec PRINCE CHOCOLAT EAGUE PAQUET ACHET USTICE DU AU CHANCE GAGNER RI JI FR BISCUITS FOURRÉS PARFUM Ingrédients Céréale farime BLE farine BLÉ complète Information Nutritionnelle Energie Matières grasses acides gras saturés Glucides sucres'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode = \"7622210449283\"\n",
    "get_big_words_from_image_clean(barcode, df = df_final)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ba1f239449f425eb3a38518ae99d1f767aed482cc07821544aaa5748f4ec966"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
