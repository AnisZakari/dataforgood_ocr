{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the input file\n",
    "\n",
    "- input file `predict_categories_dataset_ocrs.jsonl.gz` weights 316.1 MB  \n",
    "about the file : https://openfoodfacts.org/data/dataforgood2022/big/predict_categories_dataset_documentation.txt  download the file : https://openfoodfacts.org/data/dataforgood2022/big/predict_categories_dataset_ocrs.jsonl.gz\n",
    "\n",
    "- each line of the jsonl file contains OCRs associated with a barcode.\n",
    "- each OCR contains text, potentially in different languages\n",
    "\n",
    "### What is done in this notebook\n",
    "- A DataFrame is made from the jsonl.gz\n",
    "- All the OCRs of a product are concatenated\n",
    "- For each product the different languages are detected.  \n",
    "only the main language is kept (i.e the lang in which there are many words and a good confidence score)\n",
    "- As there are many languages to work with, It is suggested to work with a Multilanguage sentence transformer.\n",
    "- As the sentences can't properly be identified on a product we behave like there is only one big sentence \n",
    "- As the text may be very long for a product we need a way to extract only relevant words.\n",
    "- The approach here is to filter by language and to create a TFIDF matrix.\n",
    "- For each product of a given language we keep only the N-top words according to their TFIDF score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ocrs = os.path.abspath(\"../datasets/predict_categories_dataset_ocrs.jsonl.gz\")\n",
    "#path_products = os.path.abspath(\"../datasets/predict_categories_dataset_products.jsonl.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_from_json_for_df(json_line):\n",
    "    \"\"\" extract items from json and returns a row to put in a dataframe \"\"\"\n",
    "    code = json_line['code']\n",
    "    if \"ocrs\" in json_line:\n",
    "        texts = []\n",
    "        keys =  list(json_line['ocrs'].keys())\n",
    "        for key in keys:\n",
    "            ocr_text = json_line['ocrs'][key]['text']\n",
    "            texts.append(ocr_text)\n",
    "    row = [code, \" \".join(texts), keys]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artificial_sentence_split(text, n_words_per_sentence = 15):\n",
    "    \"\"\"splits artificially based on a pre-defined number of words. On average there are 15 words per sentence\"\"\"\n",
    "    txt_split = text.split()\n",
    "    total_words = len(txt_split)\n",
    "    if total_words >= n_words_per_sentence:\n",
    "        n_sentences = total_words // n_words_per_sentence\n",
    "        rest = total_words % n_words_per_sentence\n",
    "        chunks = [[i*n_words_per_sentence, (i+1)*n_words_per_sentence ] for i in range(n_sentences)]\n",
    "        chunks[-1][1]+=rest\n",
    "    else:\n",
    "        chunks = [[0, total_words]]\n",
    "    sentence_split = [\" \".join(txt_split[slice(*chunk)]) for chunk in chunks]\n",
    "    return sentence_split\n",
    "\n",
    "def get_clean_lists_from_fasttext(lang_labels, probs_list):\n",
    "    main_lang_idx = [np.argmax(items) for items in probs_list]\n",
    "    lang_labels_output = [lang_label[i].split('__label__')[1] for lang_label, i in zip(lang_labels, main_lang_idx)]\n",
    "    prob_list_output = [prob_list[i] for prob_list,i in zip(probs_list, main_lang_idx)]\n",
    "    return lang_labels_output, prob_list_output\n",
    "\n",
    "def text_lang_split(text:str):\n",
    "    \"\"\"\n",
    "    takes text as input and splits it in a dictionnary with languages as keys.\n",
    "    for each language we have subkeys such as:\n",
    "    text: text found with the given language\n",
    "    len_text: the length of the text\n",
    "    prob: a list of probabilities, each probability corresponds to a sentence.  \n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(\"\\.+\", \".\", text)\n",
    "    lang_dict = {}\n",
    "    sentences = artificial_sentence_split(text)\n",
    "    langs, probs = get_langs(sentences)\n",
    "    for sentence, lang, prob in zip(sentences, langs, probs):\n",
    "       \n",
    "        if lang in lang_dict:\n",
    "            lang_dict[lang][\"prob\"].append(prob)\n",
    "            lang_dict[lang][\"len_text\"] += len(sentence)\n",
    "            lang_dict[lang][\"text\"]+= sentence\n",
    "            \n",
    "        else:\n",
    "            lang_dict[lang] = {}\n",
    "            lang_dict[lang][\"prob\"] = [prob]\n",
    "            lang_dict[lang][\"len_text\"] = len(sentence)\n",
    "            lang_dict[lang][\"text\"] = sentence\n",
    "            \n",
    "    sorted_dict = {k: v for k, v in sorted(lang_dict.items(), key=lambda item: item[1][\"len_text\"], reverse = True)}\n",
    "    main_lang = next(iter(sorted_dict))\n",
    "    return main_lang, sorted_dict\n",
    "\n",
    "def get_langs(sentences):\n",
    "    lang_labels, probs_list = model.predict(sentences)\n",
    "    langs, probs = get_clean_lists_from_fasttext(lang_labels, probs_list)\n",
    "    return langs, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make DataFrame from jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(793257, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"approx time 15sec\"\"\"\n",
    "from IPython.display import clear_output\n",
    "import gzip\n",
    "import json\n",
    "# make df from json\n",
    "rows = []\n",
    "with gzip.open(path_ocrs) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 10000: # first 10 000 lines are potentially problematic we keep only further lines\n",
    "            json_line = json.loads(line)\n",
    "            row = get_row_from_json_for_df(json_line)\n",
    "            rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns = [\"code\", \"texts\", \"keys\"])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing non normalized codes (732516, 3)\n",
      "after removing short texts (643357, 3)\n"
     ]
    }
   ],
   "source": [
    "### Keep only real codes i.e codes with length = 13 characters.\n",
    "real_codes =  df[\"code\"].str.len() == 13\n",
    "df = df[real_codes]\n",
    "print(\"after removing non normalized codes\", df.shape)\n",
    "### Keep only texts > 10 char\n",
    "len_sup_10 = (df[\"texts\"].str.len()> 10)\n",
    "df = df[len_sup_10]\n",
    "print(\"after removing short texts\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract main language text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each text associated with a barcode, there are potentially many languages used to describe the product.\n",
    "The aim of this section is to find the main language and to extract its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "PRETRAINED_MODEL_PATH = '../datasets/lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "clear_output()\n",
    "\n",
    "def get_lang_items_from_pd_textlist(pdSeries) -> list:\n",
    "    text_list= []\n",
    "    lang_dict_list = []\n",
    "    main_lang_list = []\n",
    "    for t in tqdm(pdSeries):\n",
    "        sentences = artificial_sentence_split(t)\n",
    "        #detect and split text in a dict according to the languages found.\n",
    "        main_lang, lang_dict = text_lang_split(t)\n",
    "        text_to_keep = lang_dict[main_lang][\"text\"]\n",
    "        text_list.append(text_to_keep)\n",
    "        main_lang_list.append(main_lang)\n",
    "        lang_dict_list.append(lang_dict)\n",
    "    return text_list, lang_dict_list, main_lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643357/643357 [01:33<00:00, 6845.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>texts</th>\n",
       "      <th>keys</th>\n",
       "      <th>text_main_lang</th>\n",
       "      <th>main_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0041498293222</td>\n",
       "      <td>\"Percent Daily Values are based on a 2000\\nNut...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>\"Percent Daily Values are based on a 2000 Nutr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0041498310424</td>\n",
       "      <td>INGREDIENTS: WHOLE ROLLED OATS,\\nMILLED CANE S...</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>INGREDIENTS: WHOLE ROLLED OATS, MILLED CANE SU...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             code                                              texts  \\\n",
       "1   0041498293222  \"Percent Daily Values are based on a 2000\\nNut...   \n",
       "13  0041498310424  INGREDIENTS: WHOLE ROLLED OATS,\\nMILLED CANE S...   \n",
       "\n",
       "         keys                                     text_main_lang main_lang  \n",
       "1         [1]  \"Percent Daily Values are based on a 2000 Nutr...        en  \n",
       "13  [2, 1, 3]  INGREDIENTS: WHOLE ROLLED OATS, MILLED CANE SU...        en  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"approx time 2min\"\"\"\n",
    "text_list, lang_dict_list, main_lang_list = get_lang_items_from_pd_textlist(df['texts'])\n",
    "main_lang_dict = {code: dic for (code,dic) in zip(df[\"code\"], lang_dict_list)}\n",
    "#assign new items to df\n",
    "df[\"text_main_lang\"] = text_list\n",
    "df[\"main_lang\"] = main_lang_list\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "  dont_take = [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "  text_cleaned = text.replace(\"\\n\", \" \") #remove line breaks\n",
    "  text_cleaned = re.sub(\"\\S*(www\\.|\\.com|\\.net|\\.fr|\\.co\\.uk|\\.org)\\S*\", \"\", text_cleaned) #remove websites\n",
    "  #text_cleaned = re.sub(\"[^A-Za-z0-9 \\-àâäéèêëîïôöùûüÿçÂÊÎÔÛÄËÏÖÜÀÆæÇÉÈŒœÙ]\", \" \", text_cleaned) #keep alphanum and accents\n",
    "  text_cleaned = re.sub(\"\\w*([0-9]{0,}[,|\\.]{0,}[0-9])\\w*\", \" \", text_cleaned) #remove measurements \n",
    "  text_cleaned = re.sub(r\"\\b([a-zA-Z]{1})\\b\", \" \", text_cleaned) # remove isolated letters ex --> g g g g g\n",
    "  text_cleaned = re.sub(\"( +- +)\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\"[\\·|/|\\-|\\\\|(|)|\\+|\\*|\\[|\\]|™|ᴿˣ|\\*|\\—|\\^|\\\"|®|>|<|″|\\||\\&|\\#|\\,|\\;|⭐|\\xa0|\\?|\\%|\\'|©|\\@|\\$|\\€|\\:|\\}|\\{|\\°]\", \" \", text_cleaned)\n",
    "  text_cleaned = re.sub(r\" +\", \" \", text_cleaned) # remove multiple spaces\n",
    "\n",
    "  text_cleaned = \" \".join([w for w in text_cleaned.split() if (w.isalpha() and w.lower() not in dont_take)])\n",
    "  return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def parallel_calc(iterable, func, n_core = mp.cpu_count()):\n",
    "    pool = mp.Pool(n_core-1)\n",
    "    results = pool.map(func, np.array(iterable))\n",
    "    pool.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"approx time 2m\"\"\"\n",
    "#df[\"text_cleaned\"] = df[\"text_main_lang\"].apply(lambda x: text_cleaner(x))\n",
    "#df = df.set_index(\"code\", drop = True)\n",
    "\n",
    "\"\"\"approx time 30sec\"\"\"\n",
    "# simple wrapper code around text_cleaner to parallelize the work\n",
    "df[\"text_cleaned\"] = parallel_calc(df[\"text_main_lang\"], text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>texts</th>\n",
       "      <th>keys</th>\n",
       "      <th>text_main_lang</th>\n",
       "      <th>main_lang</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0041498293222</td>\n",
       "      <td>\"Percent Daily Values are based on a 2000\\nNut...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>\"Percent Daily Values are based on a 2000 Nutr...</td>\n",
       "      <td>en</td>\n",
       "      <td>Percent Daily Values are based on Nutrition Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0041498310424</td>\n",
       "      <td>INGREDIENTS: WHOLE ROLLED OATS,\\nMILLED CANE S...</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>INGREDIENTS: WHOLE ROLLED OATS, MILLED CANE SU...</td>\n",
       "      <td>en</td>\n",
       "      <td>WHOLE ROLLED OATS MILLED CANE SUGAR DRIED CRAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0041500007007</td>\n",
       "      <td>1901\\nSince\\n100% NATURA\\nClassic\\nMUSTARD\\n80...</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1901 Since 100% NATURA Classic MUSTARD 807 (22...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Since NATURA Classic MUSTARD Réf Moutarde Fren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             code                                              texts  \\\n",
       "1   0041498293222  \"Percent Daily Values are based on a 2000\\nNut...   \n",
       "13  0041498310424  INGREDIENTS: WHOLE ROLLED OATS,\\nMILLED CANE S...   \n",
       "15  0041500007007  1901\\nSince\\n100% NATURA\\nClassic\\nMUSTARD\\n80...   \n",
       "\n",
       "         keys                                     text_main_lang main_lang  \\\n",
       "1         [1]  \"Percent Daily Values are based on a 2000 Nutr...        en   \n",
       "13  [2, 1, 3]  INGREDIENTS: WHOLE ROLLED OATS, MILLED CANE SU...        en   \n",
       "15     [1, 3]  1901 Since 100% NATURA Classic MUSTARD 807 (22...        fr   \n",
       "\n",
       "                                         text_cleaned  \n",
       "1   Percent Daily Values are based on Nutrition Fa...  \n",
       "13  WHOLE ROLLED OATS MILLED CANE SUGAR DRIED CRAN...  \n",
       "15  Since NATURA Classic MUSTARD Réf Moutarde Fren...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEHCAYAAACeFSCEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeAUlEQVR4nO3df5RV5X3v8fcHiT8SjYASLgUMpLI0xKbRTBS0K7eVBNGbG2xrBJJWbkIc19WkpubGSnNvabSuJitZMXpvQxiViF0GNFYrtUZK0dQbRHRQ4y80TPwFLH8QQW2TW1Oc7/1jP4fZM5yZOXPmnH3mzPm8XLPm7Gc/e5+9j8f5+Dz72c9WRGBmZlaUMY0+ADMzay0OHjMzK5SDx8zMCuXgMTOzQjl4zMysUGMbfQBFO/roo2P69OmNPgwzs6aydevWX0TExFrsq+WCZ/r06XR2djb6MMzMmoqkF2q1L3e1mZlZoRw8ZmZWKAePmZkVysFjZmaFcvCYmVmhHDxmZlYoB4+ZmRXKwWNmZoWqW/BIWiXpVUlPlFn3ZUkh6ei0LEnXSOqS9Jikk3J1l0jann6W5Mo/LOnxtM01klSvczEzs9qpZ4vnBmB+30JJ04B5wIu54jOBmemnHViR6k4AlgOnACcDyyWNT9usAM7PbXfAe9VCROCH5ZmZ1U7dgici7gP2lFl1FXApkP9rvgC4MTIPAOMkTQbOADZExJ6I2AtsAOande+OiAciS4UbgbPrcA4OHTOzGiv0Go+kBcCuiPhpn1VTgB255Z2pbKDynWXK+3vfdkmdkjp37949jDMwM7PhKix4JL0T+HPgL4p6z5KI6IiItohomzixJpOrmplZlYps8fwmMAP4qaTnganAw5L+E7ALmJarOzWVDVQ+tUy5mZmNcIUFT0Q8HhHviYjpETGdrHvspIh4GVgHnJdGt80G3oiIl4D1wDxJ49OggnnA+rTuTUmz02i284A7ijoXMzOrXj2HU68BNgPHSdopaekA1e8CngW6gGuBCwEiYg9wBfBQ+rk8lZHqXJe2+Tnwo3qch5mZ1ZZabdRWW1tbDPQguL6fR0QgCd8mZGatTNLWiGirxb48c0EZC1duavQhmJmNWg6eMoRbN2Zm9eLgMTOzQo1t9AGMVPlrPaXXvs5jZjZ8bvHkeIocM7P6c/D0sXDlJgeQmVkdOXj68MACM7P6cvCYmVmhHDxmZlYoB4+ZmRXKwWNmZoXyfTwVyI9w8708ZmbD4xZPhRZ13N/oQzAzGxUcPBXyMGszs9pw8JiZWaEcPGZmVigHj5mZFcrBY2ZmhXLwVKg0cagnDzUzGx4HzxB45mozs+GrW/BIWiXpVUlP5Mq+KelpSY9Jul3SuNy6ZZK6JD0j6Yxc+fxU1iXpslz5DElbUvnNkg6u17nsf0/k+3nMzIapni2eG4D5fco2ACdExAeBnwHLACTNAhYBH0jbfFfSQZIOAv4GOBOYBSxOdQG+AVwVEccCe4GldTyX/Xw/j5nZ8NQteCLiPmBPn7J/ioh9afEBYGp6vQBYGxFvRcRzQBdwcvrpiohnI+LXwFpggbJ5a04Hbk3brwbOrte5mJlZ7TTyGs/ngB+l11OAHbl1O1NZf+VHAa/nQqxUXpakdkmdkjp3795do8M3M7NqNCR4JH0V2AfcVMT7RURHRLRFRNvEiROLeEszM+tH4bNTS/pvwCeAudEzPGwXMC1XbWoqo5/y14BxksamVk++fl3lR7V5pmozs6ErtMUjaT5wKfDJiPhVbtU6YJGkQyTNAGYCDwIPATPTCLaDyQYgrEuBdS9wTtp+CXBHUefhkW1mZtWr53DqNcBm4DhJOyUtBf4PcASwQdKjkr4HEBFPArcATwF3AxdFxNupNfMFYD2wDbgl1QX4M+ASSV1k13yur9e5HHBuHtlmZla1unW1RcTiMsX9hkNEXAlcWab8LuCuMuXPko16qwnfGGpmVgzPXGBmZoVy8JiZWaEcPGZmVigHj5mZFcrBY2ZmhXLwmJlZoRw8VfBD4czMqufgqZJnLzAzq46Dp0qevcDMrDoOHjMzK5SDx8zMCuXgwfO0mZkVycFjZmaFcvCYmVmhHDxmZlYoB4+ZmRXKwWNmZoVy8JiZWaEcPFXyfG1mZtVx8AyD52szMxu6ugWPpFWSXpX0RK5sgqQNkran3+NTuSRdI6lL0mOSTsptsyTV3y5pSa78w5IeT9tcI6nwydM8X5uZ2dDVs8VzAzC/T9llwMaImAlsTMsAZwIz0087sAKyoAKWA6cAJwPLS2GV6pyf267vew3K3WVmZsWrW/BExH3Anj7FC4DV6fVq4Oxc+Y2ReQAYJ2kycAawISL2RMReYAMwP617d0Q8EFlq3Jjb15AsXLmpms0AT7VjZlaNoq/xTIqIl9Lrl4FJ6fUUYEeu3s5UNlD5zjLlZUlql9QpqXP37t2917m7zMysUA0bXJBaKoU0FyKiIyLaIqJt4sSJRbylmZn1o+jgeSV1k5F+v5rKdwHTcvWmprKByqeWKR8yd5eZmRWr6OBZB5RGpi0B7siVn5dGt80G3khdcuuBeZLGp0EF84D1ad2bkman0Wzn5fZlZmYj2Nh67VjSGuB3gaMl7SQbnfZ14BZJS4EXgHNT9buAs4Au4FfAZwEiYo+kK4CHUr3LI6I0YOFCspFzhwE/Sj9mZjbC1S14ImJxP6vmlqkbwEX97GcVsKpMeSdwwnCO0czMiueZC4bJ9wKZmQ2Ng6cGhnMvkJlZq3Hw1IDvBTIzq5yDx8zMCuXgqQFf5zEzq5yDp0b8iAQzs8o4eGrE13nMzCrj4DEzs0I5eMzMrFAOHjMzK1TLBo9HoZmZNUbLBg94xgEzs0Zo6eDxSDQzs+K1dPDUkm8iNTOrjIOnhnwTqZnZ4Bw8NeSuOzOzwTl4zMysUA4eMzMrlIPHzMwK1ZLB49FnZmaNU1HwSDqtkrJKSfpTSU9KekLSGkmHSpohaYukLkk3Szo41T0kLXel9dNz+1mWyp+RdMZQjsE3j5qZNUalLZ7/XWHZoCRNAf4EaIuIE4CDgEXAN4CrIuJYYC+wNG2yFNibyq9K9ZA0K233AWA+8F1JB1V8HHUYgeZ7eczMBjdg8EiaI+nLwERJl+R+/pIsMKo1FjhM0ljgncBLwOnArWn9auDs9HpBWiatnytJqXxtRLwVEc8BXcDJwzimmvC9PGZmAxusxXMwcDhZUByR+3kTOKeaN4yIXcC3gBfJAucNYCvwekTsS9V2AlPS6ynAjrTtvlT/qHx5mW0axvfymJkNbOxAKyPiX4B/kXRDRLxQizeUNJ6stTIDeB34IVlXWd1IagfaAY455ph6vpWZmQ1iwODJOURSBzA9v01EnF7Fe34MeC4idgNIug04DRgnaWxq1UwFdqX6u4BpwM7UNXck8FquvCS/TS8R0QF0ALS1tfkCjJlZA1UaPD8EvgdcB7w9zPd8EZgt6Z3A/wPmAp3AvWTdd2uBJcAdqf66tLw5rb8nIkLSOuAHkr4N/AYwE3hwmMdmZmZ1Vmnw7IuIFbV4w4jYIulW4GFgH/AIWWvkH4G1kv4qlV2fNrke+FtJXcAespFsRMSTkm4Bnkr7uSgihhuKw1Ya1ZaNfzAzs74qDZ5/kHQhcDvwVqkwIvZU86YRsRxY3qf4WcqMSouIfwc+1c9+rgSurOYY6snhY2bWv0qDZ0n6/ZVcWQDvq+3hmJnZaFdR8ETEjHofiJmZtYaKgkfSeeXKI+LG2h6OmZmNdpV2tX0k9/pQspFoDwMOHjMzG5JKu9q+mF+WNI5s2LP1ozRfmwcYmJn1Vu1jEX5JNvOAmZnZkFR6jecfyEaxQTY56PuBW+p1UGZmNnpVeo3nW7nX+4AXImJnHY5nRCl1l/kxB2ZmtVNRV1uaLPRpspmpxwO/rudBjSR+zIGZWW1V+gTSc8nmQfsUcC6wRVJVj0VoOpFaPG70mJnVRKVdbV8FPhIRrwJImgj8Mz0Pbht19nevBSzu2MwYVTsOw8zM8ir9azqmFDrJa0PYdsSp9JpNqZut2oe7+THYZmYHqrTFc7ek9cCatLwQuKs+h1R/C1duqihM/DRRM7PaGzB4JB0LTIqIr0j6A+B30qrNwE31Prh6caCYmTXOYC2e7wDLACLiNuA2AEm/ldb91zoem5mZjUKDXaeZFBGP9y1MZdPrckQN5hFsZmb1NVjwjBtg3WE1PI4RZf+ggBoEkAcYmJn1NljwdEo6v2+hpM8DW+tzSCPD4o7NNdlPKXgcPmZmmcGu8XwJuF3SZ+gJmjbgYOD363hcDVfLAQiLOu7n5gtOq9n+zMya2YDBExGvAKdK+j3ghFT8jxFxT92PrGCDtUgiIgujKvLIo+jMzHpU+jyee4F763wsDed52czM6q8hsw9IGifpVklPS9omaY6kCZI2SNqefo9PdSXpGkldkh6TdFJuP0tS/e2Slgz7uNwyMTOru0ZNe3M1cHdEHA/8NrANuAzYGBEzgY1pGeBMYGb6aQdWAEiaACwHTgFOBpaXwsrMzEauwoNH0pHAR4HrASLi1xHxOrAAWJ2qrQbOTq8XADdG5gFgnKTJwBnAhojYExF7gQ3A/Hofv+/zMTMbnka0eGYAu4HvS3pE0nWS3kV2s+pLqc7LwKT0egqwI7f9zlTWX/kBJLVL6pTUuXv37hqeSmU8pNrMrEcjgmcscBKwIiJOBH5JT7caAJH9ha7ZX+mI6IiItohomzhx4nD2U/VReeCCmVmmEcGzE9gZEVvS8q1kQfRK6kIj/S49hmEXMC23/dRU1l/5yBSexcDMDBoQPBHxMrBD0nGpaC7wFLAOKI1MWwLckV6vA85Lo9tmA2+kLrn1wDxJ49OggnmpbMRyq8fMrPLn8dTaF4GbJB0MPAt8liwEb5G0FHiB7BHbkD335yygC/hVqktE7JF0BfBQqnd5ROwp4uCrvZnUw7XNzBoUPBHxKNnUO33NLVM3gIv62c8qYFUNjie9GO6ezMxsME37+OpaczeYmVkxHDyJu8HMzIrh4CmQ7+cxM3PwFM5dembW6hw8BXOXnpm1upYPnmofc+0528zMqtPywQO1e8y1mZkNzsFDsd1fHmBgZq3OwdMAHmBgZq3MwTMM1bZcPMDAzFqZg2eYfH3IzGxoHDzDVG3rpbu7m+7ubl/rMbOW4+CpgWqHVvtaj5m1opYOnkbfi+NrPWbWilo6eBrNQ6vNrBU5eBrM3W1m1mocPA3m7jYzazUOnhpp9PUiM7Nm0bLB46AwM2uMlg2ekcIDDMys1TQseCQdJOkRSXem5RmStkjqknSzpINT+SFpuSutn57bx7JU/oykMxp0KsPmAQZm1koa2eK5GNiWW/4GcFVEHAvsBZam8qXA3lR+VaqHpFnAIuADwHzgu5IOKujYy6q2+84DDMyslTQkeCRNBf4LcF1aFnA6cGuqsho4O71ekJZJ6+em+guAtRHxVkQ8B3QBJxdyAmZmVrVGtXi+A1wKdKflo4DXI2JfWt4JTEmvpwA7ANL6N1L9/eVltulFUrukTkmdu3fvruFpHKiaaza+zmNmraTw4JH0CeDViNha1HtGREdEtEVE28SJE+v+ftXMWL1w5SaHj5m1hEa0eE4DPinpeWAtWRfb1cA4SWNTnanArvR6FzANIK0/EngtX15mm349u/vfhn8Gg6jmmo2QBxmYWUsoPHgiYllETI2I6WSDA+6JiM8A9wLnpGpLgDvS63VpmbT+nsiaBeuARWnU2wxgJvBgQacxoGq7zjzIwMxawUi6j+fPgEskdZFdw7k+lV8PHJXKLwEuA4iIJ4FbgKeAu4GLIuLtSt6oiJtHq+luc1ebmbWCsYNXqZ+I+DHw4/T6WcqMSouIfwc+1c/2VwJX1u8Iq+fWi5lZeSOpxTOqVNvd5laPmY12Dp46qqa7zcxstHPw1JG728zMDuTgqaP93WZD7Dlzd5uZjWYOnjpzd5uZWW8Onjpzd5uZWW8OnhHK3W1mNlo5eMzMrFAOnoJUM8igu7ub7u5ut3zMbFRx8BRgOFP0eOJQMxttHDwjXfh6j5mNLq0ZPA36G15ty8etHjMbTVozeJqMh2Sb2Wji4GkCfjS2mY0mDp6CubvNzFqdg6cBqpnDzd1tZjZaOHgapJoWjLvbzGw0cPA0iFswZtaqHDwN5NkMzKwVOXia0MKVmxxAZta0Cg8eSdMk3SvpKUlPSro4lU+QtEHS9vR7fCqXpGskdUl6TNJJuX0tSfW3S1pS9Lk0ihCLOu73SDcza0qNaPHsA74cEbOA2cBFkmYBlwEbI2ImsDEtA5wJzEw/7cAKyIIKWA6cApwMLC+FVTOpdni10j9mZs2m8OCJiJci4uH0+l+BbcAUYAGwOlVbDZydXi8AbozMA8A4SZOBM4ANEbEnIvYCG4D5xZ2JmZlVo6HXeCRNB04EtgCTIuKltOplYFJ6PQXYkdtsZyrrr7zpVNvq8YwGZtaMGhY8kg4H/g74UkS8mV8X2V/Smv01ldQuqVNS51v/9nqtdjsiLFy5yeFjZk2lIcEj6R1koXNTRNyWil9JXWik36+m8l3AtNzmU1NZf+UHiIiOiGiLiLZDDh9Xs/OopYgguofegikNNDAzaxaNGNUm4HpgW0R8O7dqHVAambYEuCNXfl4a3TYbeCN1ya0H5kkanwYVzEtlTW1xx+ahbxQ99/eYmY10jWjxnAb8MXC6pEfTz1nA14GPS9oOfCwtA9wFPAt0AdcCFwJExB7gCuCh9HN5KmtuVT74za0eM2sWY4t+w4j4CfQ7DnhumfoBXNTPvlYBq2p3dCPDopX3s/aCU4e0jYdWm1mz8MwFI5AQi1beP+TRbh5kYGbNwMEzQglVdb3H4WNmI13hXW1WueF0n+XDJxvPYWY2MrjFM8JVc3NpqdWzcOWm+hyUmdkwOHiaQLXhU+0IOTOzenLwNImI6LlXZwg54mHWZjbS+BpPE1nUcT8EjFHl/78g1KvVU7re4+s+ZtYobvE0kdKjEIYzqaiv+5hZozl4mlS14ZOfXsfXfsysEdzV1sRK4SOp/7kgyihd97n5gtMAd7uZWbHc4hkFhjqzdanLbuHKTW79mFnhHDyjSGmanUpDpPRIhdIzfczMiuDgGUVKc7wNZdh1qfVjZlYUX+MZZfJzvA1l2HXpPqH9+/GwazOrE7d4RqF8K6YUKG+//faQbj4993s/cfebmdWFWzyjWET0PBo7ZUilraDS/UKlVpAkt37MrCYcPC2g7zWcUiAN5dJOadBCfgYEh5GZVcPB02JKwRH03AMUZC2bgcIkIg6Y921t+6ll6zuMzGwgvsbT4ko3oS7u2NxrRFzfa0OlVlL+n9JQ7NL6c7/3E98XZGaDcovHgJ7uuMUdm/e3WPJPQF3TPie7TqQDt8kvl1pFa9tPZcyY7P9r9rey+jyczl11Zq3JwWO97J+IlOgVLIs7NmfLue65Ur1SN11p+5LSwISIYPG1vR/jLYm17acecMOrA8ls9Gv64JE0H7gaOAi4LiK+3uBDGpXygdLrOhG9W0ml0XOl0FncsZk17XNQ9B7MEN1ZYPUNJOi5dpTX97EO+bL9x9gntPz4b7ORSc3cFy/pIOBnwMeBncBDwOKIeKq/bSa89/iYe+m1/e1v/0X3IRzDiN2mkcdXCqVyraeejTigThD84PzZvap9+toHeoXWmvPnZN2AZXa55vw5vUJq8bWbWdt+auUn0uecygVdfy20cqP+8tv217rr+55mI5GkrRHRVpN9NXnwzAH+MiLOSMvLACLir/vbZvx7j4+5l3b0t78RGyLNFjzD2qbvdzL3xzj7A99Nv2PBy20bMeDx9bduzJgxrLkg6w5c3HE/Pzh/Dp8u00Jbk1popXqSWJPCrhQk3d3dLC7zNNjStos77mftBaf1CjB3OdpIMmbMGAcPgKRzgPkR8fm0/MfAKRHxhT712oH2tHgC8EShBzpyHQ38otEHMUL4s+jhz6KHP4sex0XEEbXYUdNf46lERHQAHQCSOmuV2s3On0UPfxY9/Fn08GfRQ1JnrfbV7Pfx7AKm5ZanpjIzMxuhmj14HgJmSpoh6WBgEbCuwcdkZmYDaOqutojYJ+kLwHqy4dSrIuLJQTYrP7KgNfmz6OHPooc/ix7+LHrU7LNo6sEFZmbWfJq9q83MzJqMg8fMzArVMsEjab6kZyR1Sbqs0cdTb5KmSbpX0lOSnpR0cSqfIGmDpO3p9/hULknXpM/nMUknNfYMak/SQZIekXRnWp4haUs655vTABUkHZKWu9L66Q098BqTNE7SrZKelrRN0pxW/V5I+tP038cTktZIOrRVvheSVkl6VdITubIhfw8kLUn1t0taUsl7t0TwKJta52+AM4FZwGJJsxp7VHW3D/hyRMwCZgMXpXO+DNgYETOBjWkZss9mZvppB1YUf8h1dzGwLbf8DeCqiDgW2AssTeVLgb2p/KpUbzS5Grg7Io4HfpvsM2m574WkKcCfAG0RcQLZAKVFtM734gZgfp+yIX0PJE0AlgOnACcDy0thNaD8HFKj9QeYA6zPLS8DljX6uAr+DO4gm9PuGWByKpsMPJNerySb565Uf3+90fBDdo/XRuB04E6yOXd+AYzt+x0hGyU5J70em+qp0edQo8/hSOC5vufTit8LYAqwA5iQ/j3fCZzRSt8LYDrwRLXfA2AxsDJX3qtefz8t0eKh5wtWsjOVtYTUJXAisAWYFBEvpVUvA5PS69H+GX0HuBToTstHAa9HxL60nD/f/Z9FWv9Gqj8azAB2A99P3Y7XSXoXLfi9iIhdwLeAF4GXyP49b6U1vxclQ/0eVPX9aJXgaVmSDgf+DvhSRLyZXxfZ/6KM+vH0kj4BvBoRWxt9LCPAWOAkYEVEnAj8kp7uFKClvhfjgQVkYfwbwLs4sOupZdXze9AqwdOSU+tIegdZ6NwUEbel4lckTU7rJwOvpvLR/BmdBnxS0vPAWrLutquBcZJKN1Hnz3f/Z5HWHwm8VuQB19FOYGdEbEnLt5IFUSt+Lz4GPBcRuyPiP4DbyL4rrfi9KBnq96Cq70erBE/LTa0jScD1wLaI+HZu1TqgNPJkCdm1n1L5eWn0ymzgjVyTu6lFxLKImBoR08n+3d8TEZ8B7gXOSdX6fhalz+icVH9UtAAi4mVgh6TjUtFc4Cla8HtB1sU2W9I7038vpc+i5b4XOUP9HqwH5kkan1qQ81LZwBp9cavAi2hnkT007ufAVxt9PAWc7++QNZMfAx5NP2eR9UlvBLYD/wxMSPVFNvLv58DjZCN9Gn4edfhcfhe4M71+H/Ag0AX8EDgklR+alrvS+vc1+rhr/Bl8COhM342/B8a36vcC+BrwNNmjUv4WOKRVvhfAGrJrW/9B1hJeWs33APhc+ky6gM9W8t6eMsfMzArVKl1tZmY2Qjh4zMysUA4eMzMrlIPHzMwK5eAxM7NCOXjMzKxQDh6zJD0u4MIqt/2QpLOq3PZ5SUdXs+1wSfqxpLZGvLe1LgePWY9xQFXBQ3ZTZlXBY9ZqHDxmPb4O/KakRyV9U9JXJD2UHnz1NQBJvy9pY5o6ZLKkn0k6BrgcWJi2XVhu55IOl/R9SY+nff5hmTp/JOnBtJ+V6VlSSFohqTM9tOxrufrPS/qapIfTfo9P5e9KD/p6MM1CvSCVHyZprbIHwN0OHFbrD9FsMA4esx6XAT+PiA8BG8geenUyWWvmw5I+GhG3k00zchFwLbA8Il4E/gK4OSI+FBE397P//0U2x9VvRcQHgXvyKyW9H1gInJaO4W3gM2n1VyOiDfgg8J8lfTC36S8i4iSyh3P9j1J9srnETgZ+D/hmevzBfwd+FRHvJ3uA14eH+iGZDdfYwauYtaR56eeRtHw4WRDdB3yRbG6vByJizRD2+TGySUoBiIi9fdbPJQuCh7I5KzmMntmBz5XUTvbf7GSyJ+k+ltaVZh7fCvxB7vg/KakURIcCxwAfBa5J7/+YpNI+zArj4DErT8BfR8TKMuumkj1QbpKkMRHRXaZOte+5OiKW9SqUZpC1ZD4SEXsl3UAWJCVvpd9v0/PftIA/jIhn+uyrRodqVj13tZn1+FfgiPR6PfC59CA9JE2R9J70HJZVZI/83QZcUmbb/mwg66Ij7bPvs+k3AudIek9aP0HSe4F3kz2w7Q1Jk4AzKziX9cAX03T/SDoxld8HfDqVnUDWdWdWKAePWRIRrwGbJD0BfBz4AbBZ0uNkD0w7Avhz4P9GxE/IQufz6drMvcCsgQYXAH8FjJf0hKSfkl17yb//U8D/BP4pdYFtACZHxE/JuvyeTse0qYLTuQJ4B/CYpCfTMmTXgQ6XtI1sQISfymqF82MRzMysUG7xmJlZoTy4wKzGJH0WuLhP8aaIuKhcfbNW4642MzMrlLvazMysUA4eMzMrlIPHzMwK5eAxM7NC/X8osJ1GoPfoGwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "word_count = df['text_cleaned'].str.split().str.len()\n",
    "sns.histplot(word_count)\n",
    "plt.xlim([0,1000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_highest_scores(scores, words, n_words_to_take):\n",
    "  \"\"\"\n",
    "  takes scores and words of an OCR as inputs and returns \n",
    "  words with highest tfidf scores\n",
    "  \"\"\"\n",
    "  items = [item for item in sorted(zip(scores, words), reverse = True)]\n",
    "  best_items = items[:n_words_to_take]\n",
    "  return best_items, items\n",
    "\n",
    "def get_words_and_scores_from_tfidf_matrix(doc, cols, tfidf_matrix, index_to_word):\n",
    "  \"\"\"\n",
    "  takes a document (line of tfidf matrix) and its words (columns of tfidf matrix\n",
    "  and returns words of the documents with their ifidf scores\n",
    "  columns are the non zero values of the tfidf matrix.\n",
    "  \"\"\"\n",
    "  scores = []\n",
    "  words = []\n",
    "  for col in cols:\n",
    "    word = index_to_word[col] \n",
    "    score = tfidf_matrix[doc, col]\n",
    "    words.append(word)\n",
    "    scores.append(score)\n",
    "  return words, scores\n",
    "\n",
    "def text_selection(df_idx, sub_df_idx,  index_to_word, tfidf_matrix, n_words_to_take = 30):\n",
    "  rows, cols = tfidf_matrix[sub_df_idx].nonzero()\n",
    "  ## extract words and scores from tfidf matrix\n",
    "  words, scores = get_words_and_scores_from_tfidf_matrix(sub_df_idx, cols, tfidf_matrix, index_to_word)\n",
    "  #extract words with highest score from sentence\n",
    "  best_items, items = get_words_with_highest_scores(scores, words, n_words_to_take)\n",
    "  best_words = [item[1] for item in best_items]\n",
    "  \n",
    "  text_selection = \" \".join([word for word in df[\"text_cleaned\"].iloc[df_idx].split() if str(word).lower() in best_words])\n",
    "  text_selection = \" \".join(best_words)\n",
    "  text_selection_unique = remove_duplicates(text_selection)\n",
    "  return text_selection_unique, items\n",
    "\n",
    "def text_selection_from_Series(text_Series, tfidf_matrix, index_to_word):\n",
    "  text_selection_list = []\n",
    "  items_list = []\n",
    "  for sub_df_idx, df_idx in enumerate(tqdm(text_Series.index)):\n",
    "    text_selection_unique, items = text_selection(df_idx, sub_df_idx, index_to_word, tfidf_matrix, n_words_to_take = 30)\n",
    "    text_selection_list.append(text_selection_unique)\n",
    "    items_list.append(items)\n",
    "  return text_selection_list, items_list\n",
    "\n",
    "def get_index_to_word_dict(vectorizer):\n",
    "  index_to_word = {index: word for index, word in zip(vectorizer.vocabulary_.values(), vectorizer.vocabulary_.keys())}\n",
    "  return index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text, get_keys = False):\n",
    "    D = {word.lower(): word  for word in str(text).split()}\n",
    "    if get_keys:\n",
    "        return \" \".join(D.keys())\n",
    "    else:\n",
    "        return \" \".join(D.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tfidf_selection\"] = \"\"\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"takes approx 8min\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "problematic_langs = []\n",
    "df = df.reset_index(drop = True)\n",
    "for lang in df[\"main_lang\"].unique():\n",
    "    print(\"lang:\", lang)\n",
    "    lang_filter = df[\"main_lang\"]== lang \n",
    "    df_l = df[lang_filter]\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(min_df = 2, max_df = 0.8)\n",
    "        tfidf_matrix = vectorizer.fit_transform(df_l[\"text_cleaned\"])\n",
    "        index_to_word = get_index_to_word_dict(vectorizer)\n",
    "        text_selection_list, items_list =  text_selection_from_Series(df_l[\"text_cleaned\"], tfidf_matrix, index_to_word)\n",
    "        df.loc[lang_filter, \"tfidf_selection\"] = text_selection_list\n",
    "    except:\n",
    "        problematic_langs.append(lang)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problematic_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"tfidf_selection\"] == \"\",:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection words trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_words(lang_dict):\n",
    "    dont_take= [\"kj\", \"kcal\", \"kj\", \"total\", \"free\", \"net\", \"ingredients\", \"ingredient\", \"et\", \"de\", \"fat\", \"mg\", \"cg\", \"g\", \"kg\", \"ml\", \"cl\", \"l\", \"kl\", \"per\", \"pour\", \"valeur\", \"or\", \"le\", \"la\", \"dont\", \"consommer\", \"poids\", \"net\", \"www\", \"com\", \"which\", \"of\", \"wt\"]\n",
    "    if len(lang_dict) > 1:\n",
    "        sets = [set(remove_duplicates(text_cleaner(lang_dict[key][\"text\"]), get_keys = True).split()) for key in lang_dict.keys()]\n",
    "        return \" \".join([word for word in set.intersection(*sets) if word not in dont_take])\n",
    "    else: \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22612950508038304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"takes approx 1min30\"\"\"\n",
    "intersection_list = parallel_calc(lang_dict_list, get_intersect_words)\n",
    "print(len([item for item in intersection_list if item != \"\"]) / len(lang_dict_list))\n",
    "#pd.DataFrame(zip(df[\"code\"], intersection_list), columns = [\"barcode\", \"intersection\"]).to_csv('results_intersections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"intersection_words\"] = intersection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_selection\"] = df[\"tfidf_selection\"].astype(str) + df[\"intersection_words\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make output df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise or empty lines: 4726\n"
     ]
    }
   ],
   "source": [
    "empty_or_noise = df[\"word_selection\"].str.len() < 10\n",
    "print(\"noise or empty lines:\", empty_or_noise.sum())\n",
    "df_final = df[~empty_or_noise]\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final.to_csv(\"dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check random lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def make_barcode(x):\n",
    "    x = str(x)\n",
    "    return \"{}/{}/{}/{}\".format(x[:3], x[3:6], x[6:9], x[9:])\n",
    "\n",
    "def make_link_from_barcode(barcode, df, file = \"image\"):\n",
    "    keys = df.loc[df[\"code\"]==barcode, \"keys\"].values[0]\n",
    "    if isinstance(keys, str):\n",
    "        keys = eval(keys)\n",
    "    elif isinstance(keys, list):\n",
    "        pass\n",
    "\n",
    "    links = []\n",
    "    if file == \"image\": file = \"jpg\"\n",
    "    if file == \"json\": file = \"json\"\n",
    "    barcode_with_slash = make_barcode(barcode)\n",
    "    for key in keys:\n",
    "        link = \"https://world.openfoodfacts.org/images/products/{}/{}.{}\".format(barcode_with_slash, key,file)\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def show_images(links):\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        image_bytes = io.BytesIO(response.content)\n",
    "        img = Image.open(image_bytes)\n",
    "        img.show()\n",
    "\n",
    "def show_images_from_barcode(barcode, df):\n",
    "    links = make_link_from_barcode(barcode, df=df)\n",
    "    show_images(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3256227124426\n",
      "IngrÃ©dients\n",
      "Sucre glace, amande en poudre, blanc d'ceuf, sucre, crÃ¨me, pulpe de citron 1,8%, brisure de framboise 1,7%, beurre, blanc\n",
      "d'ceuf en poudre, chocolat 0,8% [pÃ¢te de cacao, sucre, beurre de cacao, Ã©mulsifiant: lÃ©cithine de soja], chocolat noir 0,8%\n",
      "[pÃ¢te de cacao \"Sao TomÃ©\", sucre, Ã©mulsifiant: lÃ©cithine de soja], chocolat blanc O,8% (sucre, beurre de cacao, laitentier en\n",
      "poudre, Ã©mulsifiant:lÃ©cithine de soja, arÃ´me naturel de vanille), amidon de maÃ¯s, euf entier, purÃ©e d'amande, eau, sirop de\n",
      "glucose-fructose, huile de colza, colorants :rouge de betterave-caramel ordinaire - curcumine-complexes cuivre\n",
      "chlorophylline, pistache hachÃ©e 0,3%, arÃ´me naturel de vanille, lait Ã©crÃ©mÃ© en poudre, sirop de glucose, extrait de malt\n",
      "d'orge (contient gluten), lait entier en poudre, jus de citron 0,1%, concentrÃ© de citron 0,1%, gÃ©lifiants : pectines-alginate de\n",
      "sodium, amidon transformÃ© de maÃ¯s, stabilisants : gomme de xanthane - carraghÃ©nanes, cacao maigre en poudre, cafe\n",
      "liquide 0,02%, arÃ´me naturel, sirop de sucre inverti, acidifiant: acide citrique, sÃ©questrant : phosphate disodique, grain noir\n",
      "de vanille Ã©puisÃ©e, antioxydant: acide ascorbique, huile essentielle de citron.\n",
      "Certains ingrÃ©dients de ce produit ne proviennent pas de France.\n",
      "Traces Ã©ventuelles d'arachide, de graines de sÃ©same, d'autres fruits Ã  coque et de sulfites\n",
      " ASSORTIMENT DE\n",
      "Macarons\n",
      "VANILLE\n",
      "FRAMBOISE\n",
      "Congeles\n",
      "à décongeler\n",
      "1H\n",
      "au réfrigérateur\n",
      "ASSORTIMENT DE\n",
      "Macarons\n",
      "12\n",
      "СНОСОLA . CITRON\n",
      "VANILLE PISTACHE\n",
      "Macarons\n",
      "CAFÉ · FRAMBOISE\n",
      "Entreprise\n",
      "FRANCE\n",
      "Congelés\n",
      "\n",
      "____ text after cleaning ____\n",
      "IngrÃ dients Sucre glace amande en poudre blanc ceuf sucre pulpe citron brisurede framboise beurre blanc ceuf en poudre chocolat cacao sucre beurrede cacao Ã mulsifiant lÃ cithine soja chocolat noir cacao Sao TomÃ sucre Ã mulsifiant lÃ cithine soja chocolat blanc sucre beurre cacao laitentier en poudre Ã mulsifiant lÃ cithinede soja naturel vanille amidon euf entier purÃ amande eau siropde glucose fructose huile colza colorants rouge betterave caramel ordinaire curcumine complexes cuivre chlorophylline pistachehachÃ naturel vanille lait Ã crÃ mÃ en poudre sirop glucose extrait demalt orge contient gluten lait entier en poudre jus citron concentrÃ gÃ lifiants pectines alginate sodium amidon transformÃ stabilisants gomme xanthane carraghÃ nanes cacao maigre en poudre cafe liquide naturel sirop sucre inverti acidifiant acide citrique sÃ questrant phosphate disodique grain noir vanille Ã puisÃ antioxydant acide ascorbique huile essentielle Certains ingrÃ dients ce produit ne proviennent pas TracesÃ ventuelles arachide graines sÃ same autres fruits Ã coque sulfites ASSORTIMENT DEMacarons VANILLE FRAMBOISE Congeles à décongeler au réfrigérateur ASSORTIMENT Macarons СНОСОLA CITRON VANILLE PISTACHE Macarons CAFÉ FRAMBOISE Entreprise FRANCE Congelés\n",
      "____ text selection with tdidf score\n",
      "lã mulsifiant vanille cacao poudre cithine framboise sã macarons sucre assortiment ingrã tomã questrant demacarons dients citron en puisã chocolat tracesã blanc cithinede amande brisurede lifiants naturel ceuf congeles sao\n",
      "___ text selection with intersection between languages words\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#18546\n",
    "i = np.random.choice(range(df_final.shape[0]))\n",
    "#i = 18546\n",
    "input_text = df_final[\"texts\"].iloc[i]\n",
    "text_clean = df_final[\"text_cleaned\"].iloc[i]\n",
    "text_tf = df_final[\"tfidf_selection\"].iloc[i]\n",
    "text_insct = df_final[\"intersection_words\"].iloc[i]\n",
    "barcode = df_final[\"code\"].iloc[i]\n",
    "\n",
    "\n",
    "print(barcode)\n",
    "print(input_text)\n",
    "print(\"____ text after cleaning ____\")\n",
    "print(text_clean)\n",
    "print(\"____ text selection with tdidf score\")\n",
    "print(text_tf)\n",
    "print(\"___ text selection with intersection between languages words\")\n",
    "print(text_insct)\n",
    "show_images_from_barcode(barcode, df = df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3256227124426",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/az/Documents/OpenFoodFacts/off-category-classification/dataforgood_ocr/make_df_2.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/az/Documents/OpenFoodFacts/off-category-classification/dataforgood_ocr/make_df_2.ipynb#ch0000038?line=0'>1</a>\u001b[0m dic \u001b[39m=\u001b[39m main_lang_dict[barcode]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/az/Documents/OpenFoodFacts/off-category-classification/dataforgood_ocr/make_df_2.ipynb#ch0000038?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dic\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/az/Documents/OpenFoodFacts/off-category-classification/dataforgood_ocr/make_df_2.ipynb#ch0000038?line=2'>3</a>\u001b[0m     probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(dic[key][\u001b[39m'\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 3256227124426"
     ]
    }
   ],
   "source": [
    "dic = main_lang_dict[barcode]\n",
    "for key in dic.keys():\n",
    "    probs = np.mean(dic[key]['prob'])\n",
    "    len_text = main_lang_dict[barcode][key][\"len_text\"]\n",
    "    print(key)\n",
    "    print(probs*len_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ba1f239449f425eb3a38518ae99d1f767aed482cc07821544aaa5748f4ec966"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
